{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 807ms/step - accuracy: 0.6051 - loss: 3.6858\n",
      "Epoch 2/200\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 872ms/step - accuracy: 0.6543 - loss: 2.9707\n",
      "Epoch 3/200\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 732ms/step - accuracy: 0.6582 - loss: 2.9137\n",
      "Epoch 4/200\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 737ms/step - accuracy: 0.6622 - loss: 2.8054\n",
      "Epoch 5/200\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 736ms/step - accuracy: 0.6617 - loss: 2.7597\n",
      "Epoch 6/200\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 738ms/step - accuracy: 0.6629 - loss: 2.7210\n",
      "Epoch 7/200\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 738ms/step - accuracy: 0.6562 - loss: 2.7205\n",
      "Epoch 8/200\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 738ms/step - accuracy: 0.6570 - loss: 2.6847\n",
      "Epoch 9/200\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 737ms/step - accuracy: 0.6637 - loss: 2.6206\n",
      "Epoch 10/200\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 741ms/step - accuracy: 0.6627 - loss: 2.6760\n",
      "Epoch 11/200\n",
      "\u001b[1m 50/117\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 821ms/step - accuracy: 0.6520 - loss: 2.7099"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('Conversation.csv')\n",
    "questions = df['question'].tolist()\n",
    "answers = df['answer'].tolist()\n",
    "\n",
    "'''# Sample dataset of questions and answers\n",
    "questions = [\n",
    "    \"What is your name?\",\n",
    "    \"How are you?\",\n",
    "    \"What do you do?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"What is the weather like?\"\n",
    "]\n",
    "\n",
    "answers = [\n",
    "    \"I am a chatbot.\",\n",
    "    \"I am doing great, thank you!\",\n",
    "    \"I am here to chat with you.\",\n",
    "    \"Why did the scarecrow win an award? Because he was outstanding in his field!\",\n",
    "    \"I don't know, what's your favorite weather?\"\n",
    "]'''\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "\n",
    "# Convert texts to sequences\n",
    "question_sequences = tokenizer.texts_to_sequences(questions)\n",
    "answer_sequences = tokenizer.texts_to_sequences(answers)\n",
    "\n",
    "# Pad sequences\n",
    "max_len = max(max(len(seq) for seq in question_sequences),max(len(seq) for seq in question_sequences))\n",
    "question_padded = pad_sequences(question_sequences, maxlen=max_len, padding='post')\n",
    "answer_padded = pad_sequences(answer_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# Prepare input and output\n",
    "X = question_padded\n",
    "y = answer_padded\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "embedding_dim = 128\n",
    "rnn_units = 32\n",
    "\n",
    "# Build the RNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "    tf.keras.layers.SimpleRNN(vocab_size, return_sequences=True),\n",
    "    tf.keras.layers.Dense(vocab_size,activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, np.expand_dims(y, -1), epochs=200, verbose=1)\n",
    "\n",
    "# Function to predict the answer based on the question\n",
    "def predict_answer(question):\n",
    "    question_seq = tokenizer.texts_to_sequences([question])\n",
    "    question_padded = pad_sequences(question_seq, maxlen=max_len, padding='post')\n",
    "    prediction = model.predict(question_padded)\n",
    "    predicted_word_index = np.argmax(prediction, axis=-1)[0]\n",
    "    predicted_words = [tokenizer.index_word[i] for i in predicted_word_index if i > 0]\n",
    "    return ' '.join(predicted_words)\n",
    "\n",
    "# Test the model with a sample question\n",
    "test_question = \"How are you?\"\n",
    "response = predict_answer(test_question)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready to chat! Type 'exit' to quit.\n",
      "user: hi\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Chatbot: hello\n",
      "user: you know tamil\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Chatbot: yes\n",
      "user: your education\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Chatbot: sweets\n",
      "user: your education\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Chatbot: sweets\n",
      "user: your favourite food\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Chatbot: sweets\n",
      "user: which is your favourite colour\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Chatbot: blue\n",
      "user: exit\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot is ready to chat! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    print(\"user:\",user_input)\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    response = predict_answer(user_input)\n",
    "    print(\"Chatbot:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
