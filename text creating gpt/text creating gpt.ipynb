{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: hi (3) 2nd – N.J. State 6,035 2nd – Kansas State 8,872 2nd – Missouri State 8,848 2nd – Texas State 10,722 1st – Oklahoma 10,767\n",
      "\n",
      "The following analysis examines the correlation between NCAA program results related to the national ranking of college football program rankings across four periods of the past five years.\n",
      "\n",
      "This analysis also uses an individual-adjusted model of NCAA rankings to determine college football conference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: happy to see you again.\n",
      "\n",
      "I'm all for love and light, it's just a matter of time before I see anyone with this love for you.\n",
      "\n",
      "The fact that we have a relationship is a very good thing.\n",
      "\n",
      "It's a normal thing for everyone. It's a normal thing for couples! A normal thing for couples.\n",
      "\n",
      "Our friends are often the most interesting part of relationships, how much they show you a little bit of affection.\n",
      "\n",
      "The more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: world is not enough to get people to embrace the project's ideals and work hard to achieve them. He said that by creating an organization without government involvement, it was creating an enemy.\n",
      "\n",
      "\"We want people to think more about the future and think about how to create a future to come where nothing changes and no one changes,\" he said. \"People have seen things get worse and things improved. That's why our organization is so important for what we're doing.\"\n",
      "\n",
      "According to the\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "# pip install transformers\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the chatbot using Hugging Face's pre-trained model\n",
    "chatbot = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "def chat_with_bot():\n",
    "    print(\"Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Generate a response from the model\n",
    "        response = chatbot(user_input, max_length=100, num_return_sequences=1)\n",
    "        bot_reply = response[0]['generated_text']\n",
    "        \n",
    "        # Print the bot's response\n",
    "        print(\"Bot:\", bot_reply)\n",
    "\n",
    "# Start the chat\n",
    "chat_with_bot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
